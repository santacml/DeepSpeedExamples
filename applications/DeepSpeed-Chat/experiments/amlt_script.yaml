description: Hydra-RLHF for Causal-LMs

target:
  service: aml
  name: A100-80G-PCIE-westus3 # A100-80-WUS3 # tscience-a100-80g-eastus

environment:
  registry: mcr.microsoft.com
  image: azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220714.v1
  setup:
    - pip install einops
    - pip install opt-einsum
    - pip install datasets>=2.8.0
    - pip install sentencepiece>=0.1.97
    - pip install protobuf==3.20.3
    - pip install accelerate>=0.15.0
    - pip install torch>=1.12.0
    - pip install deepspeed>=0.9.2
    - pip install transformers
    - pip install pandas
    - pip install azureml-core

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/..

storage:
    data:
        # storage_account_name: sdrgprmblob01scus
        # container_name: data
        datastore_name: babela100
        mount_dir: /mnt/data

jobs:
  - name: rlhf-sft
    sku: 80G8-A100
    command:
    - pip install -e .[all] 
    - export PATH=/home/$$(whoami)/.local/bin:$${PATH}
    - deepspeed ./examples/training/step1_supervised_finetuning/main.py
      --data_path /mnt/data/misantac_oss_rlhf/stackllama_md_processed/stackllama_md_filtered_processed_150000/
      --data_split 2,4,4
      --model_name_or_path /mnt/data/llama/llama_7b_easylm_to_hf_conversion/
      --per_device_train_batch_size 4
      --per_device_eval_batch_size 4
      --max_seq_len 512
      --learning_rate 9.65e-6
      --weight_decay 0
      --num_train_epochs 4
      --gradient_accumulation_steps 1
      --lr_scheduler_type "cosine"
      --num_warmup_steps 0
      --seed 1234
      --gradient_checkpointing
      --zero_stage 3
      --deepspeed
      --lora_dim 128
      --lora_module_name "layers."
      --output_dir=/mnt/data/deepspeed-chat/sft
    sla_tier: premium
    priority: high