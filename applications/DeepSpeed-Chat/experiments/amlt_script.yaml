description: DeepspeedChat - RLHF for Causal-LMs

target:
  service: aml
  name: A100-80G-PCIE-westus3 # A100-80-WUS3 # tscience-a100-80g-eastus

environment:
  registry: mcr.microsoft.com
  image: azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220714.v1
  setup:
    - pip install einops
    - pip install opt-einsum
    - pip install datasets>=2.8.0
    - pip install sentencepiece>=0.1.97
    - pip install protobuf==3.20.3
    - pip install accelerate>=0.15.0
    - pip install torch>=1.12.0
    - pip install deepspeed>=0.9.2
    - pip install transformers
    - pip install pandas
    - pip install azureml-core

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/..

storage:
    data:
        # storage_account_name: sdrgprmblob01scus
        # container_name: data
        datastore_name: babela100
        mount_dir: /mnt/data

jobs:
  - name: rlhf-sft
    sku: 80G8-A100
    command:
    - pip install -e .
    - export PATH=/home/$$(whoami)/.local/bin:$${PATH}
    - deepspeed ./examples/training/step1_supervised_finetuning/main.py
      --data_path=/mnt/data/misantac_oss_rlhf/stackllama_md_processed/stackllama_md_filtered_processed_150000/
      --data_split="4,2,4"
      --model_name_or_path="Llama-2-7b-hf"
      --model_dir=/mnt/data/llama-2/
      --per_device_train_batch_size=1
      --per_device_eval_batch_size=2
      --max_seq_len=800
      --learning_rate=5e-7
      --weight_decay=.1
      --num_train_epochs=5
      --gradient_accumulation_steps=3
      --lr_scheduler_type=cosine
      --num_warmup_steps=0
      --seed=42
      --zero_stage=1
      --lora_dim=0
      --lora_module_name=layers
      --deepspeed
      --only_optimize_lora
      --output_dir=/mnt/data/rlhf_models/sft
    sla_tier: premium
    priority: high

  - name: rlhf-rm
    sku: 80G8-A100
    command:
    - pip install -e .
    - export PATH=/home/$$(whoami)/.local/bin:$${PATH}
    - deepspeed ./examples/training/step2_reward_model_finetuning/main.py
      --data_path=/mnt/data/misantac_oss_rlhf/stackllama_md_processed/stackllama_md_filtered_processed_150000/
      --data_split="2,4,4"
      --model_name_or_path="Llama-2-7b-hf"
      --model_dir=/mnt/data/llama-2/
      --per_device_train_batch_size=1
      --per_device_eval_batch_size=2
      --max_seq_len=800
      --num_padding_at_beginning=0
      --learning_rate=1e-6
      --weight_decay=.1
      --num_train_epochs=3
      --gradient_accumulation_steps=12
      --lr_scheduler_type=cosine
      --num_warmup_steps=0
      --seed=42
      --zero_stage=1
      --lora_dim=0
      --lora_module_name=layers
      --deepspeed
      --only_optimize_lora
      --output_dir=/mnt/data/rlhf_models/rm
    sla_tier: premium
    priority: high

  - name: rlhf-rl
    sku: 80G8-A100
    command:
    - pip install -e .
    - export PATH=/home/$$(whoami)/.local/bin:$${PATH}
    - deepspeed --master_port 12346 ./examples/training/step3_rlhf_finetuning/main.py
      --data_path=/mnt/data/misantac_oss_rlhf/stackllama_md_processed/stackllama_md_filtered_processed_150000/
      --data_split="2,4,4"
      --actor_model_dir=/mnt/data/misantac_oss_rlhf/logs-2023-08-02-171401/
      --actor_model_name_or_path=sft
      --critic_model_dir=/mnt/data/misantac_oss_rlhf/logs-2023-07-11-104249/
      --critic_model_name_or_path=rm
      --num_padding_at_beginning=0
      --per_device_generation_batch_size=1
      --per_device_training_batch_size=3
      --generation_batches=1
      --ppo_epochs=1
      --max_answer_seq_len=400
      --max_prompt_seq_len=400
      --actor_learning_rate=5e-4
      --critic_learning_rate=5e-4
      --actor_weight_decay=0
      --critic_weight_decay=0
      --num_train_epochs=1
      --gradient_accumulation_steps=25
      --lr_scheduler_type="cosine"
      --num_warmup_steps=100
      --seed=42
      --actor_zero_stage=1
      --critic_zero_stage=1
      --actor_lora_dim=128
      --actor_lora_module_name="layers"
      --critic_lora_dim=128
      --critic_lora_module_name="layers"
      --normalize_rm_scale=1.0
      --disable_actor_dropout
      --only_optimize_lora
      --deepspeed
      --output_dir=/mnt/data/rlhf_models/rl
    sla_tier: premium
    priority: high