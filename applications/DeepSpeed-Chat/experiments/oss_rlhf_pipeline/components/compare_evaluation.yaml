$schema: https://componentsdk.azureedge.net/jsonschema/DistributedComponent.json
# $schema: http://azureml/sdk-2-0/CommandComponent.json
description: "Compare RLHF Evaluation"
display_name: Compare RLHF Evaluation
environment:
  docker: 
    # image: b5476ead59e14b909c1c0afdc3422078.azurecr.io/megatron-deepspeed-moe
    # image: babeleusrefere459829475348.azurecr.io/megatron-deepspeed-moe

    # image: mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn8-ubuntu18.04
  #   image: mcr.microsoft.com/azureml/curated/acpt-pytorch-1.11-py38-cuda11.5-gpu


    # image: mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-py38-cuda11.7-gpu
    image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
  conda:
    conda_dependencies:
      name: project_environment
      channels:
      - defaults
      dependencies:
      - python=3.8.13
      - pip:
        - einops
        - opt-einsum
        - datasets>=2.8.0
        - sentencepiece>=0.1.97
        - protobuf==3.20.3
        - accelerate>=0.15.0
        - torch>=1.12.0
        - deepspeed>=0.9.2
        - transformers>=4.29.0
        - pandas
        - azureml-core
        - rouge-score
        # - git+https://github.com/huggingface/transformers

  os: Linux
inputs: 
  baseline_model_outputs: 
    description: Path for baseline outputs
    optional: false
    default: ""
    type: path
  finetune_model_outputs: 
    description: Path for finetune outputs
    optional: false
    type: path
  num_padding_at_beginning: 
    description: num_padding_at_beginning
    optional: false
    default: 1
    type: int
  reward_model_path: 
    description: Model path for reward model, if judging reward model outputs.
    optional: true
    default: ""
    type: path
  reward_combined_mode: 
    description: Use combined actor/critic for reward
    optional: true
    default: "not_combined"
    type: string




is_deterministic: true
meta: 
  requireGpu: true
name: ds_rlhf.eval
outputs: 
  output_dir: 
    description: Output directory
    optional: false
    type: path

code: ../../../

launcher:
  type: torch.distributed
  additional_arguments: >    
    pip install -e .[['all']] && python ./examples/training/step4_eval/compare_eval.py
      --baseline_model_outputs {inputs.baseline_model_outputs} 
      --finetune_model_outputs {inputs.finetune_model_outputs} 
      --num_padding_at_beginning {inputs.num_padding_at_beginning}
      [--reward_model_path {inputs.reward_model_path}]
      [--reward_combined_mode {inputs.reward_combined_mode}]
      --output_dir {outputs.output_dir}


tags: 
  author: misantac@microsoft.com
type: DistributedComponent
version: 0.1.0