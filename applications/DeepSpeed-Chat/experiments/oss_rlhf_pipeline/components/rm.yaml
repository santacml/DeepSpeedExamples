$schema: https://componentsdk.azureedge.net/jsonschema/DistributedComponent.json
description: "RLHF RM Finetuning"
display_name: RLHF RM Finetuning
environment:
  docker: 
    image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
  conda:
    conda_dependencies:
      name: project_environment
      channels:
      - defaults
      dependencies:
      - python=3.8.13
      - pip:
        - einops
        - opt-einsum
        - datasets>=2.8.0
        - sentencepiece>=0.1.97
        - protobuf==3.20.3
        - accelerate>=0.15.0
        - torch>=1.12.0
        - deepspeed>=0.9.5
        - transformers
        - pandas
        - azureml-core
        - tensorboard

  os: Linux
inputs: 
  data_path: 
    description: Training dataset
    optional: true
    type: path
  data_split:
    description: Comma-separated list of proportions for training
    optional: false
    default: "2,4,4"
    type: string
  model_name_or_path: 
    description: Model path
    optional: true
    type: string
  model_dir: 
    description: Model dir 
    optional: true
    default: ""
    type: path
  num_padding_at_beginning: 
    description: num_padding_at_beginning
    optional: false
    default: 1
    type: int
  per_device_train_batch_size: 
    description: per_device_train_batch_size
    optional: false
    default: 4
    type: int
  per_device_eval_batch_size: 
    description: per_device_eval_batch_size
    optional: false
    default: 4
    type: int
  max_seq_len: 
    description: max_seq_len
    optional: false
    default: 512
    type: int
  learning_rate: 
    description: learning_rate
    optional: false
    default:  1e-4
    type: float
  weight_decay: 
    description: weight_decay
    optional: false
    default:  0.1
    type: float
  num_train_epochs: 
    description: num_train_epochs
    optional: false
    default: 2
    type: int
  gradient_accumulation_steps: 
    description: gradient_accumulation_steps
    optional: false
    default: 1
    type: int
  lr_scheduler_type:
    description: lr_scheduler_type
    optional: false
    default: "cosine"
    type: string
  num_warmup_steps: 
    description: num_warmup_steps
    optional: false
    default: 0
    type: int
  seed: 
    description: seed
    optional: false
    default: 42
    type: int
  zero_stage:
    description: zero_stage
    optional: false
    type: string
  lora_dim: 
    description: lora_dim
    optional: false
    default: 128
    type: int
  lora_module_name:
    description: lora_module_name
    optional: false
    default: "decoder.layers."
    type: string



is_deterministic: true
meta: 
  requireGpu: true
name: ds_rlhf.rm_train
outputs: 
  output_dir: 
    description: Output directory
    optional: false
    type: path

code: ../../../

# pip install flash-attn && 
launcher:
  type: torch.distributed
  additional_arguments: >    
    pip install -e . && python ./examples/training/step2_reward_model_finetuning/main.py
      [--data_path {inputs.data_path}  ]
      --data_split {inputs.data_split}  
      [--model_name_or_path {inputs.model_name_or_path}]
      [--model_dir {inputs.model_dir} ]
      --num_padding_at_beginning {inputs.num_padding_at_beginning} 
      --per_device_train_batch_size {inputs.per_device_train_batch_size} 
      --per_device_eval_batch_size {inputs.per_device_eval_batch_size} 
      --max_seq_len {inputs.max_seq_len}  
      --learning_rate {inputs.learning_rate}  
      --weight_decay {inputs.weight_decay}  
      --disable_dropout
      --num_train_epochs {inputs.num_train_epochs}   
      --gradient_accumulation_steps {inputs.gradient_accumulation_steps}  
      --lr_scheduler_type {inputs.lr_scheduler_type} 
      --num_warmup_steps {inputs.num_warmup_steps} 
      --seed {inputs.seed} 
      --zero_stage {inputs.zero_stage} 
      --lora_dim {inputs.lora_dim} 
      --lora_module_name {inputs.lora_module_name} 
      --only_optimize_lora
      --deepspeed 
      --output_dir {outputs.output_dir} 


tags: 
  author: aims-research
type: DistributedComponent
version: 0.1.0