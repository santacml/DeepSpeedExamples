$schema: https://componentsdk.azureedge.net/jsonschema/DistributedComponent.json
description: "RLHF PPO"
display_name: RLHF PPO
environment:
  docker: 
    image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
  conda:
    conda_dependencies:
      name: project_environment
      channels:
      - defaults
      dependencies:
      - python=3.8.13
      - pip:
        - einops
        - opt-einsum
        - datasets>=2.8.0
        - sentencepiece>=0.1.97
        - protobuf==3.20.3
        - accelerate>=0.15.0
        - torch>=1.12.0
        - deepspeed>=0.9.2
        - transformers
        - pandas
        - azureml-core
        - tensorboard
        # - git+https://github.com/huggingface/transformers

  os: Linux
inputs: 
  data_path: 
    description: Training dataset
    optional: true
    type: path
  data_split:
    description: Comma-separated list of proportions for training
    optional: false
    default: "2,4,4"
    type: string
  actor_model_name_or_path: 
    description: Model path
    optional: true
    type: string
  actor_model_dir: 
    description: Model dir 
    optional: true
    default: ""
    type: path
  critic_model_name_or_path: 
    description: Model path
    optional: true
    type: string
  critic_model_dir: 
    description: Model dir 
    optional: true
    default: ""
    type: path
  num_padding_at_beginning: 
    description: num_padding_at_beginning
    optional: false
    default: 1
    type: int
  per_device_generation_batch_size:
    description: per_device_generation_batch_size
    optional: false
    default: 4
    type: int
  per_device_training_batch_size: 
    description: per_device_training_batch_size
    optional: false
    default: 4
    type: int
  generation_batches: 
    description: generation_batches
    optional: false
    default: 4
    type: int
  ppo_epochs: 
    description: ppo_epochs
    optional: false
    default: 2
    type: int
  max_answer_seq_len: 
    description: max_answer_seq_len
    optional: false
    default: 256
    type: int
  max_prompt_seq_len: 
    description: max_prompt_seq_len
    optional: false
    default: 256
    type: int
  actor_learning_rate: 
    description: actor_learning_rate
    optional: false
    default:  1e-4
    type: float
  critic_learning_rate: 
    description: critic_learning_rate
    optional: false
    default:  1e-4
    type: float
  actor_weight_decay: 
    description: actor_weight_decay
    optional: false
    default:  0.1
    type: float
  critic_weight_decay: 
    description: critic_weight_decay
    optional: false
    default:  0.1
    type: float
  num_train_epochs: 
    description: num_train_epochs
    optional: false
    default: 1
    type: int
  gradient_accumulation_steps: 
    description: gradient_accumulation_steps
    optional: false
    default: 1
    type: int
  lr_scheduler_type:
    description: lr_scheduler_type
    optional: false
    default: "cosine"
    type: string
  num_warmup_steps: 
    description: num_warmup_steps
    optional: false
    default: 100
    type: int
  seed: 
    description: seed
    optional: false
    default: 42
    type: int
  actor_zero_stage:
    description: zero_stage
    optional: false
    type: string
  critic_zero_stage:
    description: critic_zero_stage
    optional: false
    type: string
  actor_lora_dim: 
    description: lora_dim
    optional: false
    default: 128
    type: int
  actor_lora_module_name:
    description: lora_module_name
    optional: false
    default: "decoder.layers."
    type: string
  critic_lora_dim: 
    description: lora_dim
    optional: false
    default: 128
    type: int
  critic_lora_module_name:
    description: lora_module_name
    optional: false
    default: "decoder.layers."
    type: string
  inference_tp_size: 
    description: inference_tp_size
    optional: true
    # default: 8  # when rlhf bug hybrid engine gets fixed
    default: 0 
    type: int
  tp_gather_partition_size: 
    description: tp_gather_partition_size
    optional: true
    default: 4
    type: int
  combined_mode: 
    description: Use combined actor/critic
    optional: true
    default: "not_combined"
    type: string
  stop_token:
    description: Stop generation after this token is encountered for the actor.
    optional: true
    default: ""
    type: string
  normalize_rm_scale: 
    description: normalize_rm_scale
    optional: true
    default:  0.0
    type: float
  exact_reward_function:
    description: Use exact reward
    optional: true
    type: string


is_deterministic: true
meta: 
  requireGpu: true
name: ds_rlhf.ppo
outputs: 
  output_dir: 
    description: Output directory
    optional: false
    type: path

code: ../../../

launcher:
  type: torch.distributed
  additional_arguments: >    
    pip install -e . && python ./examples/training/step3_rlhf_finetuning/main.py
      [--data_path {inputs.data_path}]
      --data_split {inputs.data_split}
      [--actor_model_name_or_path {inputs.actor_model_name_or_path}]
      [--actor_model_dir {inputs.actor_model_dir}]
      [--critic_model_name_or_path {inputs.critic_model_name_or_path}]
      [--critic_model_dir {inputs.critic_model_dir}]
      --num_padding_at_beginning {inputs.num_padding_at_beginning}
      --per_device_generation_batch_size {inputs.per_device_generation_batch_size}
      --per_device_training_batch_size {inputs.per_device_training_batch_size}
      --generation_batches {inputs.generation_batches}
      --ppo_epochs {inputs.ppo_epochs}
      --max_answer_seq_len {inputs.max_answer_seq_len}
      --max_prompt_seq_len {inputs.max_prompt_seq_len}
      --actor_learning_rate {inputs.actor_learning_rate}
      --critic_learning_rate {inputs.critic_learning_rate}
      --actor_weight_decay {inputs.actor_weight_decay}
      --critic_weight_decay {inputs.critic_weight_decay}
      --num_train_epochs {inputs.num_train_epochs}
      --gradient_accumulation_steps {inputs.gradient_accumulation_steps}
      --lr_scheduler_type {inputs.lr_scheduler_type}
      --num_warmup_steps {inputs.num_warmup_steps}
      --seed {inputs.seed}
      --actor_zero_stage {inputs.actor_zero_stage}
      --critic_zero_stage {inputs.critic_zero_stage}
      --actor_lora_dim {inputs.actor_lora_dim}
      --actor_lora_module_name {inputs.actor_lora_module_name}
      --critic_lora_dim {inputs.critic_lora_dim}
      --critic_lora_module_name {inputs.critic_lora_module_name}
      [--stop_token {inputs.stop_token}]
      [--normalize_rm_scale {inputs.normalize_rm_scale}]
      --disable_actor_dropout
      --only_optimize_lora
      --deepspeed
      [--inference_tp_size {inputs.inference_tp_size}]
      [--tp_gather_partition_size {inputs.tp_gather_partition_size}]
      [--exact_reward_function {inputs.exact_reward_function}]
      --output_dir {outputs.output_dir}

# --enable_hybrid_engine 
  # [--save_strategy {inputs.save_strategy}]


#  --actor_gradient_checkpointing \
# &> {outputs.output_dir}/training.log

tags: 
  author: misantac@microsoft.com
type: DistributedComponent
version: 0.1.0